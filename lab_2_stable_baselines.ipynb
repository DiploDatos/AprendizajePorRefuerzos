{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DiploDatos/AprendizajePorRefuerzos/blob/master/lab_2_stable_baselines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SytTR8K1oII-"
   },
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BFrF74foIJD"
   },
   "source": [
    "Créditos:\n",
    "\n",
    "* Documentación y repo de Stable-baselines https://stable-baselines3.readthedocs.io.\n",
    "    * Tutorial sobre SB3: https://github.com/araffin/rl-tutorial-jnrr19.\n",
    "* Documentación y repo de Gymnasium https://github.com/Farama-Foundation/Gymnasium. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlZ0ehGZoIJD"
   },
   "source": [
    "Stable-baselines3: framework de deep RL que provee interfaces para ejecutar y adaptar algoritmos de RL \"al estilo scikit-learn\". Permite utilizar agentes abstrayéndonos de los detalles de bajo nivel de abstracción referentes a la implementación del algoritmo$^1$\n",
    "\n",
    "Además, ofrece herramientas muy útiles como\n",
    "\n",
    "* Monitores que permiten ver el rendimiento del agente según se desempeña en el entorno, sin tener que esperar a que finalice de entrenar.\n",
    "* Callbacks que permiten accionar eventos cuando se cumplen algunas condiciones en el entrenamiento de nuestro agente (por ejemplo, detenerlo si la recompensa recibida es menor a cierto umbral tras un cierto período de tiempo).\n",
    "\n",
    "\n",
    "Documentación https://stable-baselines3.readthedocs.io\n",
    "\n",
    "Es un fork activamente mantenido de [OpenAI baselines](https://github.com/openai/baselines)\n",
    "\n",
    "La versión 3 cambia el framework subyacente de Tensorflow a Pytorch y está activamente en desarrollo; no obstante la versión 2 es completamente funcional\n",
    "\n",
    "$^1$ no obstante, al igual que sucede generalmente con librerías de ML: \n",
    "\n",
    "* Siempre es bueno tener en mente las características, ventajas y desventajas del algoritmo utilizado, pues de eso depende mucho la convergencia de nuestra solución, especialmente cuando se emplean entornos adaptados para nuestras necesidades. \n",
    "\n",
    "* Esta librería, al igual que demás frameworks generales de RL, están muy probadas en entornos estándares de RL como Atari o PyBullet. No obstante, es posible que nuestro entorno o nuestras necesidades difieran significativamente, lo que hace que en algunos casos haya que meter mano directo en el código de los algoritmos/librería."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4SWX4wuoIJE"
   },
   "source": [
    "# Interfaz básica stable-baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SD-FSSuKoIJF"
   },
   "source": [
    "### Instalación de Stable-baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ttqOLol_oIJG",
    "outputId": "e7b26d16-a6d3-45af-ca5f-332a2330427a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Instalación (no modificar)\n",
    "\n",
    "# Estamos en Colab?\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde Windows, además, instalar: \n",
    "* Microsoft Visual C++ desde https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
    "* PyType, mediante `conda install -c conda-forge pytype`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUUkScRxoIJQ"
   },
   "source": [
    "### Instalación de RLBaselinesZoo (Opcional!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiKPJUfdoIJQ"
   },
   "source": [
    "Desde Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ZM_pM0mIoIJQ",
    "outputId": "8b3aabec-28d0-4caf-93e7-4f4d6cd8725f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Instalación de RLBaselinesZoo (no modificar)\n",
    "\n",
    "# Instalamos esta lib opcional?\n",
    "install_rlzoo = False\n",
    "\n",
    "if IN_COLAB and install_rlzoo:\n",
    "    !pip install rl_zoo3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzBEoyPzoIJR"
   },
   "source": [
    "Desde Linux, ejecutando\n",
    "\n",
    "    pip install rl_zoo3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPUpOonyoIJH"
   },
   "source": [
    "## Ejecución de un algoritmo de RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Morjd1dFoIJH"
   },
   "source": [
    "### Importaciones/inicializaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OG7i44kqoIJH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from subprocess import Popen, PIPE\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "is2ytf-coIJH"
   },
   "source": [
    "### Ejemplo básico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8SqFh7noIJI",
    "outputId": "1d56db34-53bd-450a-8fe8-825f7201ffcf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x1039e21f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# MlpPolicy es una política \"estándar\" que aprende con perceptron multicapa\n",
    "# (es decir sin capas convolucionales o demás variantes),\n",
    "# 2 capas ocultas con 64 neuronas cada una\n",
    "model = DQN(\"MlpPolicy\", env)\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0f5JFC7AoIJK"
   },
   "source": [
    "### Renderización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5nIL0TuwoIJK",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jbarsce/Library/Caches/pypoetry/virtualenvs/aprendizajeporrefuerzos-7kOLcKCl-py3.9/lib/python3.9/site-packages/gymnasium/envs/classic_control/cartpole.py:215: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "if not IN_COLAB:\n",
    "    obs, info = env.reset(seed=42)\n",
    "    for i in range(1000):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        env.render()\n",
    "\n",
    "        terminated = terminated or truncated\n",
    "        if terminated:\n",
    "            obs, info = env.reset()\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTtXPFd2oIJK"
   },
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFYyMrvboIJK"
   },
   "source": [
    "#### Ver rendimiento del agente en tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pC1hqla2oIJK",
    "outputId": "77c1b83c-9952-4ee7-b4eb-2c8bfa0d2024"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x1603e4cd0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DQN(\"MlpPolicy\", gym.make(\"CartPole-v1\"), tensorboard_log=\"tensorboard/\")\n",
    "model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rldIL1u4oIJL"
   },
   "source": [
    "Para verlo en tensorboard, correr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4Db-mCxoIJL"
   },
   "source": [
    "`tensorboard --logdir=tensorboard/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "2AfqEmCWxWFv",
    "outputId": "e2297f30-400f-4480-b04e-90f1b4dc2549"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3aa5491cdab996bc\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3aa5491cdab996bc\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2u0_1dk5oIJL",
    "tags": []
   },
   "source": [
    "### Monitor\n",
    "\n",
    "Vamos a crear un monitor para loguear nuestro agente en la carpeta logs. Nuestro monitor guardará datos de recompensa (r), duración (l) y tiempo total (t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r7uF7sQdoIJL",
    "outputId": "ea0fb222-58aa-4c18-9c8c-ae97132ee3ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x1039e21c0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env = Monitor(env, \"logs/\")  # reemplazamos env por su monitor\n",
    "\n",
    "model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    ")\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yVVSGjvoIJM"
   },
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47XdjvaSoIJM",
    "outputId": "8eb9dd8d-ca66-4aee-cdbf-bce8a510ecea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=9.40 +/- 0.80\n",
      "Episode length: 9.40 +/- 0.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=9.80 +/- 0.75\n",
      "Episode length: 9.80 +/- 0.75\n",
      "New best mean reward!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x1606a2c10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "callbacks = []  # lista de callbacks a usar, pueden ser varios\n",
    "\n",
    "# callback para detener entrenamiento al alcanzar recompensa de 9.8\n",
    "# (es una recompensa muy baja, pero la establecemos a fines demostrativos)\n",
    "stop_training_callback = StopTrainingOnRewardThreshold(reward_threshold=9.8)\n",
    "\n",
    "# al crear EvalCallback, se asocia el mismo con stop_training_callback\n",
    "callbacks.append(\n",
    "    EvalCallback(\n",
    "        Monitor(env, \"logs/\"),\n",
    "        eval_freq=1000,\n",
    "        callback_on_new_best=stop_training_callback,\n",
    "    )\n",
    ")\n",
    "\n",
    "# la semilla aleatoria hace que las ejecuciones sean determinísticas\n",
    "model = DQN(\"MlpPolicy\", env, seed=42)\n",
    "model.learn(total_timesteps=10000, callback=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCPD7oaNoIJM"
   },
   "source": [
    "### Ejecutar agente RL en múltiples ambientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNiTT2AzoIJM"
   },
   "source": [
    "Esta librería provee una interfaz para ejecutar agentes en varias instancias de un mismo entorno a la vez (*vectorized environments*), de modo tal que se habilite la ejecución paralela y de otras funcionalidades útiles.\n",
    "\n",
    "Para ello, varios de sus algoritmos implementan cambios que consideren la posibilidad de que haya múltiples entornos subyacentes, por ejemplo `step(accion)` cambia a `step(lista_acciones)`, aplicando acciones a todos los entornos, recibiendo ahora múltiples observaciones y recompensas.\n",
    "\n",
    "Otro cambio: se aplica `reset()` automáticamente a cada entorno que llega a un estado final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GGZjjcDoIJN"
   },
   "source": [
    "SB brinda dos formas de utilizar entornos vectorizados:\n",
    "\n",
    "* **DummyVecEnv**, el cuál consiste en un *wrapper* de varios entornos, los cuáles funcionarán en un sólo hilo. Este wrapper es útil como entrada de algoritmos que requieren los entornos de esta forma, y habilita los procesamientos y operaciones comunes de los entornos vectorizados.\n",
    "* **SubprocVecEnv**, el cuál paraleliza multiples entornos pero en procesos de ejecucíon separados. Cada proceso tiene su propia memoria y puede adquirir derechos sobre las CPUs de la computadora donde se ejecuta. Se utiliza cuando el entorno del agente es computacionalemente complejo. Atención! **Puede comer mucha RAM**.\n",
    "\n",
    "Vemos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ovc6JDmYoIJN",
    "outputId": "6f36d2d0-bc4e-4299-b612-58103bc73f6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1606a2e20>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ejemplo de ambiente dummy, donde creamos 4 instancias del mismo\n",
    "venv = DummyVecEnv([lambda: gym.make(\"CartPole-v1\")] * 4)\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    venv,\n",
    ")\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjH4ZQ4NoIJO"
   },
   "source": [
    "### Ejecutar agente con políticas personalizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jp_aPhIdoIJO",
    "outputId": "19b3f2a7-2821-4fe9-a749-b32be258aee8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'CartPole-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.2     |\n",
      "|    ep_rew_mean     | 23.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 5137     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 26.4        |\n",
      "|    ep_rew_mean          | 26.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3540        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008014253 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | 0.000715    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.47        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    value_loss           | 40.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 34.7         |\n",
      "|    ep_rew_mean          | 34.7         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3239         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 1            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0118125845 |\n",
      "|    clip_fraction        | 0.123        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.658       |\n",
      "|    explained_variance   | 0.273        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.1         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0232      |\n",
      "|    value_loss           | 29.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 47.9        |\n",
      "|    ep_rew_mean          | 47.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3130        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014418518 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.62       |\n",
      "|    explained_variance   | 0.305       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.3        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0225     |\n",
      "|    value_loss           | 50.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 64.6         |\n",
      "|    ep_rew_mean          | 64.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3076         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068466053 |\n",
      "|    clip_fraction        | 0.0545       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.605       |\n",
      "|    explained_variance   | 0.348        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.3         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0133      |\n",
      "|    value_loss           | 43.7         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1606a2970>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos una clase con una red neuronal de 128x128 neuronas\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\", \n",
    "    policy_kwargs=dict(net_arch=[128, 128]), \n",
    "    env=\"CartPole-v1\", \n",
    "    verbose=1\n",
    ")\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Buen post](https://medium.com/aureliantactics/understanding-ppo-plots-in-tensorboard-cbc3199b9ba2) en donde se explica qué significan varias de estas métricas. Para verlas en detalle podemos consultar directamente el [código fuente](https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/ppo/ppo.py), que está bien documentado y no es muy difícil de seguir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbwkBi87oIJO"
   },
   "source": [
    "### Utilizar un entorno personalizado\n",
    "\n",
    "Antes que nada, además de la interfaz que ya vimos de Gym, hay otras nociones que tenemos que tener en cuenta en este contexto:\n",
    "\n",
    "* Los entornos definen un espacio de estados y de acciones, a partir de los cuáles los modelos asumen y respetan la \"forma\" de observaciones y acciones. Por ejemplo, algunos algoritmos están diseñados para espacios de acciones discretos (DQN), continuos (DDPG) o bien poseen implementaciones particulares pueden usarse en ambos (PPO, en el repo de SB3). En cuanto a los espacios, algunos algoritmos asumen explícitamente un espacio discreto (y pequeño), como Q-Learning, mientras que otros como PPO asumen cualquier tipo de espacio.\n",
    "* Los dos tipos más comunes de estados o acciones son los espacios discretos `gym.spaces.Discrete` y los continuos `gym.spaces.Box`.\n",
    "* Los espacios discretos definen un conjunto de $n$ estados/acciones $\\{ 0, 1, \\dots, n-1 \\}$, mientras que los espacios continuos definen un espacio $\\mathbb{R}^d$, de una de las siguientes 4 formas: $[a, b], (-\\infty, b], [a, \\infty), (-\\infty, \\infty)$, en donde $a,b$ son las cotas superior e inferior (de existir).\n",
    "* Ejemplos: un espacio de acciones `Discrete(4)` tiene 4 acciones: $\\{0,1,2,3\\}$; un espacio de estados `Discrete(16)` tiene 16 estados. Un espacio de estados ALTURA, ANCHO, N_CANALES que represente una imagen RGB acotada en $[a=0, b=255]$ se puede crear como\n",
    "\n",
    "`observation_space = spaces.Box(low=0, high=255, shape=(HEIGHT, WIDTH, N_CHANNELS), dtype=np.uint8)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFLZieLNoIJO"
   },
   "source": [
    "Para usar un entorno compatible por esta librería, el mismo tiene que heredar de *gym.Env*. Vemos un ejemplo (crédito: Antonin Raffin https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/5_custom_gym_env.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "str0v6DUoIJO"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class GoLeftEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Ambiente personalizado que sigue la interfaz de gym.\n",
    "    Es un entorno simple en el cuál el agente debe aprender a ir siempre\n",
    "    hacia la izquierda.\n",
    "    \"\"\"\n",
    "\n",
    "    # Dado que podemos estar en colab, no podemos implementar la salida\n",
    "    # por interfaz gráfica ('human' render mode)\n",
    "    metadata = {\"render.modes\": [\"console\"]}\n",
    "    # Definimos las constantes\n",
    "    LEFT = 0\n",
    "    RIGHT = 1\n",
    "\n",
    "    def __init__(self, grid_size=10):\n",
    "        super(GoLeftEnv, self).__init__()\n",
    "\n",
    "        # Tamaño de la grilla de 1D\n",
    "        self.grid_size = grid_size\n",
    "        # Inicializamos en agente a la derecha de la grilla\n",
    "        self.agent_pos = grid_size - 1\n",
    "\n",
    "        # Definimos el espacio de acción y observaciones\n",
    "        # Los mismos deben ser objetos gym.spaces\n",
    "        # En este ejemplo usamos dos acciones discretas: izquierda y derecha\n",
    "        n_actions = 2\n",
    "        self.action_space = spaces.Discrete(n_actions)\n",
    "        # La observación será la coordenada donde se encuentra el agente\n",
    "        # puede ser descrita tanto por los espacios Discrete como Box\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=self.grid_size, shape=(1,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self, seed=None) -> Tuple[np.array, dict]:\n",
    "        \"\"\"\n",
    "        Reinicia el ambiente y devuelve la observación inicial\n",
    "\n",
    "        Importante: la observación devuelta debe ser un array de numpy\n",
    "        :return: (np.array, dict)\n",
    "        \"\"\"\n",
    "        # Se inicializa el agente a la derecha de la grilla\n",
    "        self.agent_pos = self.grid_size - 1\n",
    "\n",
    "        # convertimos con astype a float32 (numpy) para hacer más general\n",
    "        # el agente (en caso de que querramos usar acciones continuas)\n",
    "        return (np.array([self.agent_pos]).astype(np.float32), {})\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == self.LEFT:\n",
    "            self.agent_pos -= 1\n",
    "        elif action == self.RIGHT:\n",
    "            self.agent_pos += 1\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Se recibió una acción inválida={action} que no es parte del\\\n",
    "                    espacio de acciones\"\n",
    "            )\n",
    "\n",
    "        # Evitamos que el agente se salga de los límites de la grilla\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\n",
    "\n",
    "        # Llegó el agente a su estado objetivo (izquierda) de la grilla?\n",
    "        terminated = bool(self.agent_pos == 0)\n",
    "        truncated = False  # no limitamos la duración de los episodios\n",
    "\n",
    "        # Asignamos recompensa sólo cuando el agente llega a su objetivo\n",
    "        # (recompensa = 0 en todos los demás estados)\n",
    "        reward = 1 if self.agent_pos == 0 else 0\n",
    "\n",
    "        # gym también nos permite devolver información adicional, ej. en Atari:\n",
    "        # las vidas restantes del agente (no usaremos esto por ahora)\n",
    "        info = {}\n",
    "\n",
    "        return (\n",
    "            np.array([self.agent_pos]).astype(np.float32),\n",
    "            reward,\n",
    "            terminated,\n",
    "            truncated,\n",
    "            info,\n",
    "        )\n",
    "\n",
    "    def render(self, mode=\"console\"):\n",
    "        if mode != \"console\":\n",
    "            raise NotImplementedError()\n",
    "        # en nuestra interfaz de consola, representamos el agente como una\n",
    "        # cruz, y el resto como un punto\n",
    "        print(\".\" * self.agent_pos, end=\"\")\n",
    "        print(\"x\", end=\"\")\n",
    "        print(\".\" * (self.grid_size - self.agent_pos))\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validamos que el ambiente cumpla con la interfaz de gym\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "env = GoLeftEnv()\n",
    "# Si el entorno no cumple con la interfaz, se lanzará una excepción\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "flB9-j1SoIJP",
    "outputId": "67ccb533-c91c-441a-8f85-72346c10e1ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 5285     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 55.6        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 4027        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019783046 |\n",
      "|    clip_fraction        | 0.378       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.677      |\n",
      "|    explained_variance   | 0.445       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0123      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0449     |\n",
      "|    value_loss           | 0.158       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 22.4        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3684        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028993802 |\n",
      "|    clip_fraction        | 0.436       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.613      |\n",
      "|    explained_variance   | 0.416       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0229      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0501     |\n",
      "|    value_loss           | 0.139       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 13.2        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3550        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037439898 |\n",
      "|    clip_fraction        | 0.317       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.488      |\n",
      "|    explained_variance   | -0.0528     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0601     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.04       |\n",
      "|    value_loss           | 0.03        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 12.2        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3457        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046875235 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.358      |\n",
      "|    explained_variance   | -1.08       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000579   |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    value_loss           | 0.00376     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 11.2         |\n",
      "|    ep_rew_mean          | 1            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3425         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071321996 |\n",
      "|    clip_fraction        | 0.0921       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.292       |\n",
      "|    explained_variance   | 0.00648      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0111      |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0176      |\n",
      "|    value_loss           | 0.000881     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 10.1         |\n",
      "|    ep_rew_mean          | 1            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3389         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053308844 |\n",
      "|    clip_fraction        | 0.0839       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.22        |\n",
      "|    explained_variance   | 0.602        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0267      |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0229      |\n",
      "|    value_loss           | 0.000412     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.36        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3370        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031726122 |\n",
      "|    clip_fraction        | 0.0541      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0945     |\n",
      "|    explained_variance   | 0.82        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0127     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    value_loss           | 0.000167    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 9.24         |\n",
      "|    ep_rew_mean          | 1            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3344         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007793694 |\n",
      "|    clip_fraction        | 0.0154       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.06        |\n",
      "|    explained_variance   | 0.927        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00146      |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00888     |\n",
      "|    value_loss           | 4.64e-05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9.12        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3344        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000665435 |\n",
      "|    clip_fraction        | 0.0123      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0406     |\n",
      "|    explained_variance   | 0.952       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0103     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00801    |\n",
      "|    value_loss           | 3.31e-05    |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "env = GoLeftEnv(grid_size=10)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1).learn(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos agente entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........x.\n",
      "Box(0.0, 10.0, (1,), float32)\n",
      "Discrete(2)\n",
      "0\n",
      "Step 1\n",
      "obs= [8.] reward= 0 done= False\n",
      "........x..\n",
      "Step 2\n",
      "obs= [7.] reward= 0 done= False\n",
      ".......x...\n",
      "Step 3\n",
      "obs= [6.] reward= 0 done= False\n",
      "......x....\n",
      "Step 4\n",
      "obs= [5.] reward= 0 done= False\n",
      ".....x.....\n",
      "Step 5\n",
      "obs= [4.] reward= 0 done= False\n",
      "....x......\n",
      "Step 6\n",
      "obs= [3.] reward= 0 done= False\n",
      "...x.......\n",
      "Step 7\n",
      "obs= [2.] reward= 0 done= False\n",
      "..x........\n",
      "Step 8\n",
      "obs= [1.] reward= 0 done= False\n",
      ".x.........\n",
      "Step 9\n",
      "obs= [0.] reward= 1 done= True\n",
      "x..........\n",
      "Objetivo cumplido! reward=1\n"
     ]
    }
   ],
   "source": [
    "obs, _ = env.reset()\n",
    "env.render()\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "print(env.action_space.sample())\n",
    "\n",
    "GO_LEFT = 0\n",
    "# Agent óptimo: siempre va hacia la izquierda!\n",
    "n_steps = 20\n",
    "for step in range(n_steps):\n",
    "    print(f\"Step {step + 1}\")\n",
    "    obs, reward, terminated, truncated, info = env.step(GO_LEFT)\n",
    "    done = terminated or truncated\n",
    "    print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(\"Objetivo cumplido!\", f\"reward={reward}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgrWKJlsoIJT"
   },
   "source": [
    "## Normalización de features y recompensas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIfn3o99oIJT"
   },
   "source": [
    "Stable-Baselines3 tiene predefinidos un conjunto de **wrappers** genericos que pueden utilizarse para preprocesar las observaciones que llegan al agente RL, desacoplando del mismo el prepocesamiento.\n",
    "\n",
    "Entre las funcionalidades disponibles tenemos:\n",
    "* **VecFrameStack**: Se utiliza cuando la observación que percibe el agente es una imagen. Sirve para expandir el espacio de estados apilando N frames de manera conjunta (ver [paper de DQN](https://arxiv.org/abs/1312.5602) para más detalle).\n",
    "* **VecNormalize**: Se utiliza para normalizar las observaciones y/o las recompenzas que percibe el agente, a $\\mu=0$ y $\\sigma=1$. También permite cortar valores de observaciones y/o recompensas que excedan un rango establecido. \n",
    "* **VecCheckNan**: Se utiliza para trackear los estados del entorno que generan que los gradientes de la NN se hagan NaN.\n",
    "* **VecVideoRecorder**: Se utiliza para exportar el funcionamiento de la política aprendida por el agente a un video (MP4).\n",
    "\n",
    "Ademas, se pueden crear **wrappers** personalizados extendiendo la clase **VecEnvWrapper**:\n",
    "\n",
    "```\n",
    "class MiWrapper(VecEnvWrapper):\n",
    "    [...]\n",
    "```\n",
    "### Ejemplo (VecNormalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3VOh1HQCVJNA",
    "outputId": "b9f70740-a806-4978-988d-aa296af53c58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x160793250>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "# Multiples entornos vectorizados. Normalizamos tanto observaciones como\n",
    "# recompensas, y recortamos ambas cuando exceden 10\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecNormalize(\n",
    "    env, norm_obs=True, norm_reward=True, clip_obs=10.0, clip_reward=10.0\n",
    ")\n",
    "\n",
    "model = DQN(\"MlpPolicy\", env)\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exel4Gr8oIJQ",
    "tags": []
   },
   "source": [
    "# RL-baselines3-zoo\n",
    "\n",
    "Colección de agentes RL y herramientas útiles para ejecutarlos, evaluarlos e incluso hacer videos con ellos. Los agentes de este repo están preparados con la configuración requerida para los distintos tipos de entornos, incluyendo Atari, PyBullet y entornos clásicos, incluyendo configuraciones e híper-parámetros que producen buenas políticas para tales entornos.\n",
    "\n",
    "Esta librería ofrece un muy buen punto de partida para utilizar agentes / entornos personalizados, ya que ofrece una [interfaz](https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/train.py) fácilmente adaptable a nuestras necesidades.\n",
    "\n",
    "Si se usan entornos personalizados con rl-baselines3-zoo, debe tenerse en cuenta que se deben definir todos los híper-parámetros de antemano, sea al instanciar el agente, o en la carpeta */rl-baselines3-zoo/hyperparams*; de lo contrario arrojará error por no encontrar qué híper-parámetro usar.\n",
    "\n",
    "**Importante:** esta librería es enteramente opcional, no es necesaria para realizar los labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jbarsce/Library/Caches/pypoetry/virtualenvs/aprendizajeporrefuerzos-7kOLcKCl-py3.9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    import rl_zoo3\n",
    "    zoo_installed = True\n",
    "except ImportError as e:\n",
    "    zoo_installed = False\n",
    "\n",
    "zoo_installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6TCyu_UoIJR"
   },
   "source": [
    "## Ejecución\n",
    "\n",
    "Los agentes pueden ser llamados desde la consola mediante comandos como\n",
    "\n",
    "`python train.py --algo algo_name --env env_id`\n",
    "\n",
    "Los cuales pueden ser llamados usando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "jBLMVxhIoIJR"
   },
   "outputs": [],
   "source": [
    "if zoo_installed:\n",
    "    # lo siguiente es equivalente a ejecutar en la terminal:\n",
    "    # python -m rl_zoo3.train --algo ppo --env CartPole-v1\n",
    "\n",
    "    args = [\"--algo\", \"ppo\", \"--env\", \"CartPole-v1\"]\n",
    "\n",
    "    p = Popen(\n",
    "        [\"python\", \"-m\", \"rl_zoo3.train\"] + args, stdin=PIPE, stdout=PIPE, stderr=PIPE\n",
    "    )\n",
    "    output, err = p.communicate()\n",
    "    rc = p.returncode\n",
    "    os.chdir(cwd)\n",
    "    assert rc == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40H4hNcqoIJR"
   },
   "source": [
    "Ver en acción el agente entrenado (nota: no disponible en Google Colab, requiere ffmpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "cNija96loIJR"
   },
   "outputs": [],
   "source": [
    "if not IN_COLAB and zoo_installed:\n",
    "    # lo siguiente es equivalente a ejecutar en la terminal:\n",
    "    # python -m rl_zoo3.enjoy --algo ppo --env CartPole-v1 --folder logs/\n",
    "\n",
    "    args = [\"--algo\", \"ppo\", \"--env\", \"CartPole-v1\", \"--folder\", \"logs/\"]\n",
    "\n",
    "    p = Popen(\n",
    "        [\"python\", \"-m\", \"rl_zoo3.enjoy\"] + args, stdin=PIPE, stdout=PIPE, stderr=PIPE\n",
    "    )\n",
    "    output, err = p.communicate()\n",
    "    rc = p.returncode\n",
    "    os.chdir(cwd)\n",
    "    assert rc == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSfIitK7oIJR"
   },
   "source": [
    "También es posible grabar un video! Ver https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#record-a-video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Phq975BYoIJR"
   },
   "source": [
    "Ver curva de aprendizaje obtenida por el agente desde *utils.plot*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Q8EA41oQoIJS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jbarsce/.pyenv/versions/3.9.13/lib/python3.9/runpy.py:127: RuntimeWarning: 'rl_zoo3.plots.plot_train' found in sys.modules after import of package 'rl_zoo3.plots', but prior to execution of 'rl_zoo3.plots.plot_train'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "# equivalente a ejecutar en la terminal:\n",
    "# python -m rl_zoo3.plots.plot_train --algo ppo --env CartPole-v1 --exp-folder logs/\n",
    "\n",
    "args = [\"--algo\", \"ppo\", \"--env\", \"CartPole-v1\", \"--exp-folder\", \"logs/\"]\n",
    "\n",
    "p = Popen([\"python\", \"-m\", \"rl_zoo3.plots.plot_train\"] + args, stdout=PIPE)\n",
    "output, err = p.communicate()\n",
    "rc = p.returncode\n",
    "os.chdir(cwd)\n",
    "\n",
    "assert rc == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Figure(640x480)\\n'\n"
     ]
    }
   ],
   "source": [
    "print(output) # TODO decode this output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWzAHj6qoIJT",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Híper-parámetros\n",
    "\n",
    "rl-baselines-zoo provee híper-parámetros que resultan en curvas de aprendizaje que convergen en buena cantidad de entornos. Estos híper-parámetros pueden verse en cada uno de los archivos YAML de cada algoritmo, [acá](https://github.com/DLR-RM/rl-baselines3-zoo/tree/master/hyperparams).\n",
    "\n",
    "También provee funcionalidad para optimizar los híper-parámetros con la librería [Optuna]( https://github.com/optuna/optuna). En los mismos se incluyen rangos de híper-parámetros que se usaron para optimizar entornos como los de PyBullet, y son fácilmente modificables para adaptarlo a nuestros propios entornos. Para ver cómo se llama a la interfaz de Optuna ver [este código](https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/utils/hyperparams_opt.py).\n",
    "\n",
    "Nota: **Optuna come muchos recursos!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenando otros agentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Algunos conjuntos de entornos\n",
    "\n",
    "CartPole es una excelente línea base (de hecho suele ser la prueba preliminar de todo nuevo algoritmo), porque tiene recompensas constantes pero requiere cierta solidez por parte del algoritmo para hacerlo converger al óptimo.\n",
    "\n",
    "No obstante, al implementar un algoritmo, es deseable que el mismo pueda desenvolverse de forma consistente en varios grupos de entornos. A continuación va una lista con varios entornos que sirven como prueba:\n",
    "\n",
    "| Entornos                                                                                                           | Estados            | Acciones            | Dificultad      | Implementado por                                                                                                                                                            |\n",
    "|--------------------------------------------------------------------------------------------------------------------|--------------------|---------------------|-----------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Clásicos (CartPole, MountainCar, Pendulum, Deep sea (testea exploración), umbrella (testea asignación de crédito)) | Discretos/Continuos          | Continuas/discretas | Baja/Media      | [Gym](https://github.com/openai/gym/wiki/Table-of-environments) y [BSuite](https://github.com/deepmind/bsuite)                                                                                                                          |\n",
    "| Grilla (desde pequeñas donde hay que salir hasta grandes con muchas habitaciones y subproblemas)                   | Discretos/imágenes | Discretas           | Baja/Media/Alta | [gym-minigrid](https://github.com/maximecb/gym-minigrid)                                                                                                                    |\n",
    "| Grilla/plataforma/estilo Atari, generados proceduralmente                                                          | Imágenes           | Discretas           | Media/Alta      | [Gym](https://github.com/openai/procgen)                                                                                                                                    |\n",
    "| Plataforma 2D, como LunarLander o BipedalWalker                                                                    | Continuos          | Continuas/discretas | Media/Alta      | [Gym](https://github.com/openai/gym/wiki/Table-of-environments)                                                                                                                                                                         |\n",
    "| Primera persona en 3D                                                                                              | Imágenes           | Continuas/discretas | Media/Alta      | [Deepmind](https://github.com/deepmind/lab)                                                                                                                                 |\n",
    "| Simulación física de pequeños robots                                                                               | Continuos          | Continuas           | Media/Alta      | Gym (con el motor [MuJoCo](https://mujoco.org/) o [PyBullet](https://docs.google.com/document/d/10sXEhzFRSnvFcl3XxNGhnD4N2SedqwdAvK3dsihxVUA/edit#heading=h.wz5to0x8kqmr)) |\n",
    "| Atari                                                                                                              | Continuos          | Discretas           | Media/Alta      | [Gym](https://github.com/openai/gym/wiki/Table-of-environments) (son todos aquellos entornos que terminan en \"*-v4*\"                                                                                                                                                                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Resumen de algunos algoritmos\n",
    "\n",
    "Se resumen ahora varios algoritmos del estado del arte de aprendizaje por refuerzos profundo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Algoritmo | Tipo       | Espacio de acciones | Resumen rápido                                                                                                                                                | Artículo                         |\n",
    "|-----------|------------|---------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------|\n",
    "| DQN       | Off-policy | Discretas           | Extiende Q-Learning a deep learning. En Stable-baselines, DQN incluye todas las mejoras ya incorporadas.                                                      | https://arxiv.org/abs/1312.5602  |\n",
    "| ACER      | Off-policy | Discretas           | Combina una arquitectura actor-critic con un buffer y repetición de experiencia.                                                                              | https://arxiv.org/abs/1611.01224 |\n",
    "| A3C       | On-policy  | Ambos               | Múltiples agentes corriendo en múltiples instancias del ambiente, acumulando sus gradientes y actualizándolos tras un cierto tiempo.                          | https://arxiv.org/abs/1602.01783 |\n",
    "| PPO       | On-policy  | Ambos               | Punta de lanza de policy gradient, incluye mecanismo para que el gradiente actualice de forma acotada, mejorando drásticamente la estabilidad.             | https://arxiv.org/abs/1707.06347 |\n",
    "| DDPG      | Off-policy | Continuas           | Como en espacios de acciones continuos es muy difícil encontrar $\\max_a Q(s,a)$, se aproxima via $Q(s, a(s \\mid \\theta_a))$, siendo $a$ un actor estocástico. | https://arxiv.org/abs/1509.02971 |\n",
    "| TD3       | Off-policy | Continuas           | Mejora DDPG utilizando dos funciones $Q$ y retrasando la actualización para reducir la sobreestimación de $Q$.                                                | https://arxiv.org/abs/1802.09477 |\n",
    "| SAC       | Off-policy | Continuas           | Usa dos funciones $Q$, introduce el bonus por entropía y usa un actor estocástico que muestrea acciones según una política $\\pi$.                             | https://arxiv.org/abs/1801.01290 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Resumen de algunas herramientas/trucos comúnmente usados\n",
    "\n",
    "* Experience replay/buffer de experiencia: guarda las experiencias en un buffer para poder usarlas repetidamente durante el entrenamiento.\n",
    "    * Ventajas: experiencias raras pero muy relevantes (ej: que tienen mucho error de actualización) quedan guardadas en memoria, pudiendo ser usadas repetidamente para aprender sin necesidad de esperar a que se repitan.\n",
    "    \n",
    "    En métodos de gradiente de política, en cambio, el aprendizaje queda reflejado en los pesos, lo cuál puede hacer que un agente no se desenvuelva correctamente en entornos de recompensa escasa como MountainCar.\n",
    "    \n",
    "    * Desventajas: requiere considerable RAM, son usables solamente en algoritmos off-policy y su muestreo no necesariamente refleja la probabilidad real de tener esas experiencias en el entorno (añadiendo sesgo), por lo que es recomendable usarlo junto con importance sampling.\n",
    "    \n",
    "* Importance sampling: aplica un descuento a las actualizaciones a partir de experience replay relacionado a cuán probable era realizar esa transición.\n",
    "* Entropía: añade un bonus a la función de recompensa para que bonifique políticas $\\pi(s \\mid a)$ que tengan mayor entropía que otras, motivando la exploración del agente.\n",
    "* Juntar varias secuencias de imágenes. Usado principalmente en entornos de Atari para poder evaluar la dirección de movimientos.\n",
    "* Normalización de recompensas/estados. Normaliza las recompensas y observaciones usualmente con una media móvil, de modo tal que las observaciones/recompensas reflejen su relación y varianza con respecto a las demás.\n",
    "* Clipping (recorte) de recompensas. Se usaba principalmente en entornos de Atari con algoritmos tipo DQN para recortar el impacto que las distintas recompensas tenían, a una constante (ej: 1). Suele usarse como una cota máxima de recompensas/observaciones normalizadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Lab 2\n",
    "\n",
    "1. Crear tu propio entorno y entrenar agentes RL en el mismo. Analizar la convergencia con distintos algoritmos* (ej: PPO, DQN), resultados con distintas funciones de recompensa e híper-parámetros. \n",
    "\n",
    "    Algunas ideas:\n",
    "\n",
    "    * Transformar GoLeftEnv en una grilla 2D, añadir paredes / trampas / agua.\n",
    "    * Crear un entorno que juegue a algún juego como el ta-te-ti.\n",
    "    * Crea un entorno totalmente nuevo que sea de tu interés!\n",
    "\n",
    "2. Entrena agentes en entornos más complejos con librerías de agentes más avanzadas. La idea es que den rienda suelta a su creatividad. Algunas ideas de agentes / entornos:\n",
    "\n",
    "* stable-baselines/rl-baselines-zoo (como lo vimos en este notebook).\n",
    "* (CleanRL)[https://github.com/vwxyzjn/cleanrl]\n",
    "* Pueden inspirarse en [DQN](https://huggingface.co/learn/deep-rl-course/unit3/hands-on), [Policy Gradient](https://huggingface.co/learn/deep-rl-course/unit4/hands-on) o [demás secciones](https://huggingface.co/learn/deep-rl-course/unit0/introduction) del curso de RL de HuggingFace.\n",
    "* Alguna aplicación o notebook usando [Transformer RL](https://github.com/huggingface/trl).\n",
    " \n",
    "\n",
    "Tener en cuenta:\n",
    "\n",
    "* Google Colab tiene una limitante en cuanto a cantidad de recursos de CPU/GPU (incluido un \"rendimiento decreciente silencioso\"), lo cuál reduce la capacidad de entrenar distintos entornos.\n",
    "* Si el entorno no está implementado en stable-baselines, debe hacerse un wrapper a mano, lo que puede ser sencillo o puede llevar algo más de trabajo, teniendo que tocar código subyacente de la librería. \n",
    "\n",
    "\\* pueden ser usando los agentes de stable-baselines, de rl-baselines-zoo, o bien utilizando algún otro algoritmo (incluso tabular)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5YbRAbyoIJT"
   },
   "source": [
    "# Recursos adicionales\n",
    "\n",
    "* [Excelente curso de HuggingFace para aprender más de deep RL](https://huggingface.co/learn/deep-rl-course)\n",
    "* [Framework adicional de aprendizaje por refuerzos a gran escala](https://docs.ray.io/en/master/rllib.html)\n",
    "* [Awesome Deep RL](https://github.com/kengz/awesome-deep-rl)\n",
    "* [Comunidad de Bots de RL para Rocket League](https://rlbot.org/)\n",
    "* [Blog de Lilian Weng de Deep RL, robótica y NLP](https://lilianweng.github.io/lil-log/)\n",
    "* [The Nuts and Bolts of Deep RL Research](http://joschu.net/docs/nuts-and-bolts.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwXMoJy9oIJT"
   },
   "source": [
    "FIN"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "SD-FSSuKoIJF",
    "uUUkScRxoIJQ"
   ],
   "include_colab_link": true,
   "name": "stable_baselines.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "aprendizajeporrefuerzos-7kOLcKCl-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
